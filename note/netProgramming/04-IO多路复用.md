# table of contents

- [五种IO模型](#五种io模型)
- [阻塞IO](#阻塞io)
- [非阻塞IO](#非阻塞io)
- [IO多路复用](#io多路复用)
  - [select](#select)
  - [poll](#poll)
  - [epoll](#epoll)
    - [epoll工作模式](#epoll工作模式)
- [信号驱动式I/O](#信号驱动式io)
- [异步IO](#异步io)

## [五种IO模型](#table-of-contents)

有五种网络IO模型：阻塞I/O (blocking I/O)，非阻塞I/O (non-blocking I/O)，I/O复用 (I/O multiplexing)，信号驱动式I/O (signal-driven I/O) 和 异步I/O (asynchronous I/O)。

1. 阻塞I/O模型：最传统的一种IO模型，即在读写数据过程中会发生阻塞现象。当用户线程发出IO请求之后，内核会去查看数据是否就绪，如果没有就绪就会等待数据就绪，而用户线程就会处于阻塞状态，用户线程交出CPU。当数据就绪之后，内核会将数据拷贝到用户线程，并返回结果给用户线程，用户线程才解除block状态。

2. 非阻塞I/O模型：在阻塞IO模式有一个缺点是每次io事件没有就绪时，用户进程需要一直等待。因此引入了非阻塞IO。当用户线程发起一个read操作后，并不需要等待，而是马上就得到了一个结果。如果结果是一个error时，它就知道数据还没有准备好，于是就返回到用户进程去执行其他任务，等过一段时间后在去查看数据是否准备好。一旦内核中的数据准备好了，并且又再次收到了用户线程的请求，那么它马上就将数据拷贝到了用户线程，然后返回。

3. I/O复用模型：多路复用IO主要用于处理多个IO连接时候的场景。在多路复用IO模型中，会有一个线程不断去轮询多个socket的状态，只有当socket真正有读写事件时，才真正调用实际的IO读写操作。因为在多路复用IO模型中，只需要使用一个线程就可以管理多个socket，系统不需要建立新的进程或者线程，也不必维护这些线程和进程，并且只有在真正有socket读写事件进行时，才会使用IO资源，所以它大大减少了资源占用。

4. 信号驱动式I/O模型：当用户线程发起一个IO请求操作，会给对应的socket注册一个信号函数，然后用户线程会继续执行，当内核数据就绪时会发送一个信号给用户线程，用户线程接收到信号之后，便在信号函数中调用IO读写操作来进行实际的IO请求操作。

5. 异步I/O模型：异步IO模型才是最理想的IO模型，在异步IO模型中，当用户线程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从内核的角度，当它受到一个asynchronous read之后，它会立刻返回，说明read请求已经成功发起了，因此不会对用户线程产生任何block。然后内核会等待数据准备完成，然后将数据拷贝到用户线程，并且当这一切都完成之后内核会给用户线程发送一个信号告诉它read操作完成了。

## [阻塞IO](#table-of-contents)

```text
阻塞IO模型是最传统的IO模型，即在读写数据过程中会发生阻塞现象。当用户线程发出IO请求之后，内核会去查看数据是否就绪，如果没有就绪就会等待数据就绪，而用户线程就会处于阻塞状态，用户线程交出CPU。当数据就绪之后，内核会将数据拷贝到用户线程，并返回结果给用户线程，用户线程才解除block状态。
```

> 常见的阻塞IO函数

```c
// 阻塞读
ssize_t read(int fd, void *buf, size_t count);
// 阻塞写
ssize_t write(int fd, const void *buf, size_t count);
```

## [非阻塞IO](#table-of-contents)

```text
在阻塞IO模式有一个缺点是每次io事件没有就绪时，用户进程需要一直等待。因此引入了非阻塞IO。当用户线程发起一个read操作后，并不需要等待，而是马上就得到了一个结果。如果结果是一个error时，它就知道数据还没有准备好，于是就返回到用户进程去执行其他任务，等过一段时间后在去查看数据是否准备好。一旦内核中的数据准备好了，并且又再次收到了用户线程的请求，那么它马上就将数据拷贝到了用户线程，然后返回。
```

> 常见的非阻塞IO函数

```c
// 设置文件描述符为非阻塞
int flags = fcntl(fd, F_GETFL, 0);
fcntl(fd, F_SETFL, flags | O_NONBLOCK);
// 非阻塞读
ssize_t read(int fd, void *buf, size_t count);
// 非阻塞写
ssize_t write(int fd, const void *buf, size_t count);
```

## [IO多路复用](#table-of-contents)

> IO多路复用是指内核一旦发现进程指定的一个或者多个IO条件准备读取，它就通知该进程。IO多路复用适用于同时监听多个文件描述符的情况，它比多进程和多线程技术更加高效和方便。
> linux下实现IO多路复用的系统调用有select、poll和epoll。

```text
select, poll 和 epoll 都是 I/O 多路复用的技术。I/O 多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但 select，poll，epoll 本质上都是同步 I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。

select 的优点是可移植性更好，在某些 Unix 系统上不支持 poll。select 对于超时值提供了更好的精度：微秒，而 poll 是毫秒。但 select 的缺点是单个进程可监视的 fd 数量被限制，即能监听端口的数量有限。当 socket 较多时，每次 select 都要通过遍历 FD_SETSIZE 个 socket，不管是否活跃，这会浪费很多 CPU 时间。

poll 的优点是没有最大文件描述符数量的限制。但与 select 类似，poll 也需要轮询文件描述符集合，并在用户态和内核态之间进行拷贝，在文件描述符很多的情况下开销会比较大。

epoll 是 Linux 下多路复用 IO 接口 select/poll 的增强版本。相对于 select 和 poll 来说，epoll 更加灵活，没有描述符限制。epoll 使用一个文件描述符管理多个描述符，将用户关心的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的 copy 只需一次。epoll 支持水平触发和边沿触发两种模式。
```

### [select](#table-of-contents)

> [select](/code/04-linux网络编程/35-IO多路复用/select.c)

```c
#include <sys/select.h>

int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);

// 用于初始化 fd_set 结构体
void FD_ZERO(fd_set *fdset);
// 用于设置监听的文件描述符
void FD_SET(int fd, fd_set *fdset);
// 用于清除监听的文件描述符
void FD_CLR(int fd, fd_set *fdset);
// 用于判断文件描述符是否就绪
int FD_ISSET(int fd, fd_set *fdset);

// timeval 结构体 用于设置超时时间以及返回剩余时间
struct timeval {
    long tv_sec; /* seconds */
    long tv_usec; /* microseconds */
};

// fd_set 结构体用于设置监听的文件描述符以及返回就绪的文件描述符
struct fd_set {
    // 1024/(8*8)=16 个 long 类型 128 个字节 1024 位 1024 个文件描述符
    unsigned long fds_bits[FD_SETSIZE / (8 * sizeof(long))];
};

#define FD_SETSIZE 1024
```

### [poll](#table-of-contents)

> [poll](/code/04-linux网络编程/35-IO多路复用/poll.c)

```c
#include <poll.h>

int poll(struct pollfd *fds, nfds_t nfds, int timeout);

// pollfd 结构体用于设置监听的文件描述符以及返回就绪的文件描述符
struct pollfd {
    int fd; /* 文件描述符 */
    short events; /* 等待的事件 */
    short revents; /* 实际发生了的事件 */
};
```

| 事件 | 常值 | events | revents | 描述 |
| --- | --- | --- | --- | --- |
| 读事件 | POLLIN | 是 | 是 | 有数据可读 |
| 读事件 | POLLRDNORM | 是 | 是 | 有普通数据可读 |
| 读事件 | POLLRDBAND | 是 | 是 | 有优先数据可读 |
| 读事件 | POLLPRI | 是 | 是 | 有紧急数据可读 |
| 写事件 | POLLOUT | 是 | 是 | 可以写数据 |
| 写事件 | POLLWRNORM | 是 | 是 | 可以写普通数据 |
| 写事件 | POLLWRBAND | 是 | 是 | 可以写优先数据 |
| 错误事件 | POLLERR | 否 | 是 | 发生错误 |
| 错误事件 | POLLHUP | 否 | 是 | 发生挂起 |
| 错误事件 | POLLNVAL | 否 | 是 | 描述符不是一个打开的文件 |

### [epoll](#table-of-contents)

> [epoll](/code/04-linux网络编程/35-IO多路复用/epoll.c)

```c
#include <sys/epoll.h>

// 创建一个 epoll 句柄 返回一个文件描述符用于监听事件 用于初始化 epoll 实例
int epoll_create(int size);
// 用于控制 epoll 实例的事件注册
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
// 用于等待事件的发生
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);

// epoll_event 结构体用于设置监听的文件描述符以及返回就绪的文件描述符
struct epoll_event {
    uint32_t events; /* Epoll events */
    epoll_data_t data; /* User data variable */
};

// epoll_data_t 结构体用于设置监听的文件描述符以及返回就绪的文件描述符
typedef union epoll_data {
    void *ptr;
    int fd;
    uint32_t u32;
    uint64_t u64;
} epoll_data_t;

// events 事件类型
#define EPOLLIN 0x001 /* 可读事件 */
#define EPOLLPRI 0x002 /* 优先事件 */
#define EPOLLOUT 0x004 /* 可写事件 */
#define EPOLLRDNORM 0x040 /* 普通数据可读事件 */
#define EPOLLRDBAND 0x080 /* 优先数据可读事件 */
#define EPOLLWRNORM 0x100 /* 普通数据可写事件 */
#define EPOLLWRBAND 0x200 /* 优先数据可写事件 */
#define EPOLLMSG 0x400 /* 消息可读事件 */
#define EPOLLERR 0x008 /* 错误事件 */
#define EPOLLHUP 0x010 /* 挂起事件 */
#define EPOLLRDHUP 0x2000 /* 对端关闭连接事件 */
#define EPOLLWAKEUP (1 << 29) /* 唤醒事件 */
#define EPOLLONESHOT (1 << 30) /* 一次性事件 */
#define EPOLLET (1 << 31) /* 边缘触发事件 */

// op 操作类型
#define EPOLL_CTL_ADD 1 /* 注册新的 fd 到 epoll 实例中 */
#define EPOLL_CTL_MOD 2 /* 修改已经注册的 fd 的监听事件 */
#define EPOLL_CTL_DEL 3 /* 从 epoll 实例中删除一个 fd */

// maxevents 用于设置 epoll_wait 返回的最大事件数
```

> epoll 与 select、poll 的区别

```text
1. select 和 poll 都需要使用者自己不断地轮询所有被监听的 socket，直到有数据到达或者超时，而 epoll 只需要 epoll_wait 一次就可以了。

2. select 和 poll 每次调用都需要把 fd 集合从用户态往内核态拷贝一次，并且要把 current 转换到被监听的进程的上下文，而 epoll 只需要通过一次系统调用把 fd 集合传入内核，避免了这个开销。

3. select 和 poll 都是水平触发，而 epoll 既可以是水平触发，也可以是边缘触发。
```

#### [epoll工作模式](#table-of-contents)

> epoll 有两种工作模式：[LT（level trigger）](/code/04-linux网络编程/36-epoll工作模式/epoll_lt.c)和 [ET（edge trigger）](/code/04-linux网络编程/36-epoll工作模式/epoll_et.c)。

```text
LT模式：
    当epoll_wait检测到描述符事件到达时，将此事件通知进程，进程可以不立即处理该事件，下次调用epoll_wait时，会再次响应进程并通知此事件。

    LT（level - triggered）是缺省的工作方式，并且同时支持 block 和 no-block socket。在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的 fd 进行 IO 操作。如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。

ET模式：
    当epoll_wait检测到描述符事件到达时，将此事件通知进程，进程必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应进程并通知此事件。

    ET（edge - triggered）是高速工作方式，只支持 no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK错误)。ET 模式在很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。
```

## [信号驱动式I/O](#table-of-contents)

```text
在信号驱动IO模型中，应用程序使用系统调用（如 sigaction() ）注册一个信号处理函数，用于处理IO操作完成时的信号。应用程序发起一个IO操作后，可以继续执行其他任务，而不会被IO操作阻塞。当数据准备完成后，操作系统会发送一个信号给应用程序，应用程序接收到信号后，就可以在信号处理函数中调用IO操作的系统调用，将数据从内核拷贝到用户空间。
```

```text
优点：

1. 异步性：应用程序可以在IO操作进行的同时执行其他任务，不需要等待IO操作完成。

2. 简单：相对于其他异步IO模型，信号驱动IO模型的编程接口相对简单，使用起来更为方便。

缺点：

1. 信号处理函数的编写：编写信号处理函数需要特殊的处理和注意事项，例如需要注意信号安全性（Signal Safety）和可重入性（Reentrancy），以确保程序的正确性。

2. 只能捕捉一个信号：每个进程只能同时捕捉一个信号，当应用程序需要处理多个IO事件时，可能需要额外的处理逻辑。

3. 并发性受限：信号驱动IO模型对并行处理支持有限，因为异步通知的信号只能逐个地处理。
```

```text
需要注意的是，对于同一种信号，每个进程只能有一个信号处理函数。当同一种信号连续发生时，信号处理函数只会被调用一次。这可能会导致在信号处理函数中处理多个连续发生的IO事件的复杂性。

如果应用程序长时间没有处理信号，或者存在处理信号的时间过长的情况，可能会导致信号堆积和延迟，从而影响应用程序的性能。
```

## [异步IO](#table-of-contents)

```text
异步IO是指内核一旦发现进程指定的IO操作可以无阻塞地执行时，就会通知进程执行IO操作，而不必进程自己负责等待。

异步IO的特点是：当进程发起IO操作之后，就直接返回再也不管了，等到内核发送IO完成的信号，进程再来处理这个信号就好了。也就是说，异步IO的特点是通知机制，当进程发起IO请求之后，就直接返回，不等待内核IO操作的完成，当内核完成IO操作之后，会给进程发送信号，通知进程IO操作完成。

异步IO的优点是：相对于同步IO，异步IO不需要进程自己去等待IO操作的完成，不会因为IO操作而阻塞。但是异步IO的缺点是：实现比较复杂。
```

```text
1. AIO（Linux）：Linux提供了AIO机制，通过使用 aio_read() 和 aio_write() 等函数，应用程序可以使用异步方式进行IO操作。在AIO中，应用程序提交IO请求后可以继续执行其他任务，而不需要等待IO操作完成。当IO操作完成时，操作系统会通知应用程序，从而实现异步IO。需要注意的是，AIO在Linux上的使用较为复杂，需要特定的库和配置支持。

2. IOPC（Windows）：Windows操作系统支持IOPC机制，通过使用IO完成端口（IO Completion Port）来实现异步IO。应用程序可以将IO请求提交到IO完成端口，然后继续执行其他任务。当IO操作完成时，操作系统会将完成的IO请求排队到IO完成端口上。应用程序可以使用 GetQueuedCompletionStatus() 函数来获取完成的IO请求，并处理相应的结果。
```
